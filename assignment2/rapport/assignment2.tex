\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{bm}
\usepackage[colorlinks=true]{hyperref}


\usepackage{amsmath,amssymb,amsfonts,amsthm}
\newcommand{\mbf}[1]{\bm{#1}}

\title{Assignment 2}
\author{Sebastian Paaske Tørholm \& Kristoffer Søholm}

\begin{document}
\maketitle


\section{Question 1.3}
We wish to find $p(w|t)$ and know that this is proportional to $p(w)p(t|w)$,
so we ``complete the square'' by looking at the exponential term:
\begin{align*}
    &\exp\left(-\frac{1}{2}(w-m_0)^T S_0^{-1}(w-m_0)\right)
    \prod_{n=1}^N \exp\left(-\frac{\beta}{2}(t_n-w^T\phi(x_n))^2\right)\\
    = &\exp\left(-\frac{1}{2}(w-m_0)^T S_0^{-1}(w-m_0) +
    \sum_{n=1}^N -\frac{\beta}{2}(t_n-w^T\phi(x_n))^2\right)\ .
\end{align*}
By looking at all the terms depending on $w$ in this formula, we can find the
mean and covariance matrix of $p(w|t)$. From the above formula we see, that
these terms are exactly
\begin{align*}
    &-\frac{1}{2}(w^TS_0^{-1}w - 2w^TS_0^{-1}m_0) -\frac{\beta}{2}
    \sum_{n=1}^N \left(-2t_nw^T\phi(x_n) + (w^T\phi(x_n))^2\right)\\
    = &-\frac{1}{2}(w^TS_0^{-1}w - 2w^TS_0^{-1}m_0) -\frac{\beta}{2}\left(
    -2w^T\Phi(x)^T t + w^T\Phi(x)^T\Phi(x)w\right) \\
    = &-\frac{1}{2}(w^TS_0^{-1}w - 2w^TS_0^{-1}m_0)
    +\beta w^T\Phi^T t -\frac{\beta}{2} w^T\Phi^T\Phi w \\
    = &\ w^T(S_0^{-1}m_0 + \beta\Phi^T t) - \frac{1}{2}w^T(S_0^{-1}
    + \beta\Phi^T\Phi)w\ .
\end{align*}
From this we immediately get
\[
    \Sigma^{-1} = S_0^{-1} + \beta\Phi^T\Phi\ ,
\]
and we also know that the linear coefficient of $w$ above is equal to
$\Sigma^{-1}\mu$, so we get
\[
    \mu = \Sigma(S_0^{-1}m_0 + \beta\Phi^t t)\ .
\]
We have now proved eqn.~3.49 of \cite{Bishop}, which is what we wanted to
do.\qed

\section{Question 2.3}
We are given the function $d : \mathbb{R}^2 \times \mathbb{R}^2 \rightarrow \mathbb{R}$ defined as
$$d(\bm{x},\bm{z}) = \| \bm{Mx} - \bm{Mz} \|_{2} = \| \bm{M}(\bm{x} - \bm{z}) \|_{2}$$

where
$$\bm{M} = \begin{pmatrix} 1 & 0 \\ 0 & 10 \end{pmatrix}$$

and $\bm{x},\bm{y} \in \mathbb{R}^2$. In order to be a metric, the function $d$ must satisfy the following conditions:

\begin{enumerate}
    \item $d(\bm{x},\bm{z}) \geq 0$
    \item $d(\bm{x},\bm{z}) = 0$\ \ \ iff $x = z$
    \item $d(\bm{x},\bm{z}) = d(\bm{z},\bm{x})$
    \item $d(\bm{x},\bm{z}) \leq d(\bm{x},\bm{y}) + d(\bm{y},\bm{z})$
\end{enumerate}

All of the conditions follow directly from the definition of a norm, which has the
following properties:

\begin{enumerate}
    \setcounter{enumi}{4}
    \item $\| a\bm{x}\| = |a|\| \bm{x} \|$\ \ where $a \in \mathbb{R}$
    \item $\| \bm{x} \| \geq 0$
    \item $\| \bm{x} \| = 0$\ \ \ iff $\bm{x} = 0$
    \item $\| \bm{x} + \bm{y} \| \leq \| \bm{x} \| + \| \bm{y} \|$
\end{enumerate}

Because $d$ is a norm, it is trivial that $(1)$ follows from $(6)$ and $(2)$ follows from $(7)$. Because of $(5)$ we get that 

\begin{align*}
    d(\bm{x}, \bm{z}) &= \| \bm{M}(\bm{x} - \bm{z}) \|_{2} \\
                      &= \| \bm{M}(-1\cdot(\bm{z} - \bm{x})) \|_{2} \\
                      &= |-1|\| \bm{M}(\bm{z} - \bm{x}) \|_{2} \tag{per 5}\\
                      &= \| \bm{M}(\bm{z} - \bm{x}) \|_{2} \\
                      &= d(\bm{z},\bm{x})
\end{align*}

which implies $(3)$. Finally, we need to check $(4)$ where we use

\begin{align*}
d(\bm{x}, \bm{y}) + 
d(\bm{y}, \bm{z}) &= \| \bm{M}(\bm{x} - \bm{y}) \|_{2} + \| \bm{M}(\bm{y} - \bm{z}) \|_{2} \\
                  &\geq \| \bm{M}(\bm{x} - \bm{y}) +  \bm{M}(\bm{y} - \bm{z}) \|_{2} \tag{per 8}\\
                  &= \| \bm{M}(\bm{x} - \bm{z}) \|_{2} \\
                  &= d(\bm{x},\bm{z})
\end{align*}


\begin{thebibliography}{9}
    \bibitem{Bishop}
        C. M. Bishop,
        \emph{Pattern Recognition and Machine Learning},
        Springer Verlag,
        2006.
\end{thebibliography}

\end{document}
